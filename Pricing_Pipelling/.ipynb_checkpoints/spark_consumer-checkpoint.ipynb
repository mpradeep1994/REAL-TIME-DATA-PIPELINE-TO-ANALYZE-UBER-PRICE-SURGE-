{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "import json\n",
    "import ast\n",
    "import datetime\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from google.cloud import bigtable\n",
    "from google.cloud import happybase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dict={\n",
    "    'A':(37.787817,-122.396595),\n",
    "    'B':(37.776908,-122.394825),\n",
    "    'C':(37.810470, -122.476624),\n",
    "    'D':(37.775505, -122.446444),\n",
    "    'E':(37.810604,-122.409856),\n",
    "    'F':(37.687802, -122.470648), \n",
    "    'G':(37.737842, -122.431546), \n",
    "    'H':(37.768719, -122.488666),\n",
    "    'G':(37.621206, -122.388759),\n",
    "    'I':(37.692554, -122.432361)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars C:\\spark-2.0.0-bin-hadoop2.7\\jars\\spark-examples_2.9.3-0.7.1-hadoop2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars C:\\spark-2.0.0-bin-hadoop2.7\\jars\\hbase-0.90.0.jar.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\Users\\Pradeep\\Google Drive\\Uber_Pricing_Pipelling\\UBER-Price-Surge-fe496f50d763.json\"\n",
    "#client_credentials = json.load(open(\"<path to .json credentials>\"))\n",
    "project_id = 'uber-price-surge'\n",
    "instance_id = 'uber-price' \n",
    "table_name = 'price_table'\n",
    "cluster = 'uber-price-cluster'\n",
    "zone = 'us-east1-b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection(project_id, instance_id ):\n",
    "    client = bigtable.Client(project=project_id, admin=True)\n",
    "    instance = client.instance(instance_id)\n",
    "    connection = happybase.Connection(instance=instance)\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = connection(project_id, instance_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_rows(connection,table_name,value):\n",
    "    try:\n",
    "        column_family_name = 'cf1'\n",
    "        print('Inserting values into the {} table.'.format(table_name))\n",
    "        table = connection.table(table_name)\n",
    "        column_name = '{fam}:price'.format(fam=column_family_name)\n",
    "        table.put(value[0], {column_name:json.dumps(value[1])})\n",
    "    finally:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"test\"\n",
    "ssc=StreamingContext(sc,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"hbase.client.connection.impl\": \"com.google.cloud.bigtable.hbase1_1.BigtableConnection\",\n",
    "    \"google.bigtable.project.id\": project_id,\n",
    "    \"google.bigtable.zone.name\": zone,\n",
    "    \"google.bigtable.cluster.name\":cluster,\n",
    "    \"hbase.mapred.outputtable\": table_name,\n",
    "    \"mapreduce.outputformat.class\": \"org.apache.hadoop.hbase.mapreduce.TableOutputFormat\",\n",
    "    \"mapreduce.job.output.key.class\": \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "    \"mapreduce.job.output.value.class\": \"org.apache.hadoop.io.Writable\"\n",
    "}\n",
    "keyConv = \"org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter\"\n",
    "valueConv = \"org.apache.spark.examples.pythonconverters.StringListToPutConverter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranformation(rdd):    \n",
    "    datamap = rdd.map(lambda x: mapjson(x))\n",
    "    json=json.dumps(rdd)\n",
    "    val_dict = ast.literal_eval(json)\n",
    "    distance = val_dict['distance']\n",
    "    high= val_dict['high_estimate']\n",
    "    low= val_dict['low_estimate']\n",
    "    avg = (int(high)+int(low))/2\n",
    "    key = val_dict['key']\n",
    "    uber_type= val_dict['display_name']\n",
    "    pkey=key+ str(datetime.datetime.now()) \n",
    "    resp=(pkey,[distance,high,low,avg,uber_type])\n",
    "    put_rows(connection,table_name,resp)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveRecord(rdd):  \n",
    "    datamap = rdd.map(lambda x: mapjson(x))\n",
    "    datamap.saveAsNewAPIHadoopDataset(conf=conf,keyConverter=keyConv,valueConverter=valueConv) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapjson(price):\n",
    "    z=price[1].encode('ascii')\n",
    "    k=price[0].encode('ascii')\n",
    "    #json=json.dumps(price[1])\n",
    "    val_dict = ast.literal_eval(z)\n",
    "    dic_values={  \n",
    "                'key' : val_dict['key'],\n",
    "                'distance': val_dict['distance'],\n",
    "                'low_estimate':val_dict['low_estimate'],\n",
    "                'high_estimate': val_dict['high_estimate'],\n",
    "                'price' : (int(val_dict['low_estimate'])+int(val_dict['high_estimate']))/2,\n",
    "                'uber_type': val_dict['display_name']             \n",
    "               }\n",
    "    \n",
    "    key = val_dict['key'][:3]\n",
    "    pkey=key+ str(datetime.datetime.now()) \n",
    "    resp=(str(pkey),[str(pkey),\"cf1\",\"uber_json\",json.dumps(dic_values)])\n",
    "    \n",
    "    return resp   \n",
    "    \"\"\" pkey=k+\"|\"+ str(datetime.datetime.now())\n",
    "    z = ast.literal_eval(z)\n",
    "    resp=(pkey,[pkey,\"cf1\",\"cf1\",\"uber_price_json\",json.dumps(z)])\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zkQuorum, topic = 'localhost:2181','test'\n",
    "kvs=KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})\n",
    "lines = kvs.map(lambda x: [x[0],x[1]])\n",
    "lines.foreachRDD(SaveRecord) \n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan():\n",
    "    table = connection.table(table_name)\n",
    "    n=table.scan()\n",
    "    for i in n:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('asd', {'cf1:greeting': '[\"asd\", \"aswd\"]'})\n",
      "('aswd', {'cf1:greeting': '[\"asd\", \"aswd\"]'})\n",
      "('greeting0', {'cf1:greeting': '[\"asd\", \"asd\"]'})\n",
      "('greeting1', {'cf1:greeting': '[\"asd\", \"asd\"]'})\n",
      "('greeting2', {'cf1:greeting': '[\"asd\", \"asd\"]'})\n",
      "('r1', {'cf1:c1': 'test-value'})\n"
     ]
    }
   ],
   "source": [
    "scan()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
